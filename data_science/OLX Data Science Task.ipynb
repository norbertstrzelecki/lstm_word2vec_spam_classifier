{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Task <a class='tocSkip'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *olx_spam_data__training_set.csv* contains ads data from the Real Estate category with a label indicating if itâ€™s spam.\n",
    "\n",
    "**Your task is to build and train a model that allows to identify spam content based on the provided ad parameter.**\n",
    "\n",
    "The output of your work should be a script that we can run in Jupyter notebooks and which would allow us to run your model on our validation set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import morfeusz2\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the automatic validation please put the file named ``olx_spam_data__validation_set.csv`` in the ``data`` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "training_set = 'data/olx_spam_data__training_set.csv'\n",
    "df_train = pd.read_csv(training_set, sep=None, engine='python', encoding='utf-8')\n",
    "print('Training set ready.')\n",
    "\n",
    "# Validation Set\n",
    "try:\n",
    "    validation_set = 'data/olx_spam_data__validation_set.csv'\n",
    "    df_val = pd.read_csv(validation_set, sep=None, engine='python', encoding='utf-8')\n",
    "    print('Validation set ready.')\n",
    "except FileNotFoundError:\n",
    "    print('Validation set not found. Check the name and path of the file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping two useless columns\n",
    "df_train.drop(['ID', 'Unnamed: 5'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution of classes to assess the baseline for classifier\n",
    "print('Mean value:', df_train['SPAM_FLAG'].mean())\n",
    "print('Distribution:')\n",
    "print(df_train['SPAM_FLAG'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization using Morfeusz library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SGJP dictionary\n",
    "morf = morfeusz2.Morfeusz(dict_path='./data', dict_name='sgjp')\n",
    "morf.dict_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function for automatic lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(df, col):\n",
    "    '''\n",
    "    Arguments:\n",
    "    df: DataFrame with columns to be lemmatized,\n",
    "    col: Column name to be lemmatized, eg. 'DESCRIPTION'\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Generating list from column values\n",
    "    raw_ = [str([art]) for art in df[col]]\n",
    "\n",
    "    # Small letters \n",
    "    lower_ = [art.lower() for art in raw_]\n",
    "\n",
    "    # Tokenization\n",
    "    tokenized_ = [nltk.word_tokenize(art) for art in lower_]\n",
    "\n",
    "    # Removing punctuation chars\n",
    "    no_punc_ = [[token for token in art if token not in string.punctuation] for art in tokenized_]\n",
    "\n",
    "    # Removing stopwords\n",
    "    stopwords = './data/polishstopwords.txt'\n",
    "    no_stopw_ = [[token for token in art if not token in stopwords] for art in no_punc_]\n",
    "\n",
    "    # Lemmatization\n",
    "    morf = morfeusz2.Morfeusz(expand_tags=False, dict_path='./data/', dict_name='sgjp')\n",
    "    lemmatized__ = [[morf.analyse(token)[0][2][1] for token in art] for art in no_stopw_]\n",
    "\n",
    "    # Removing artifacts after lemmatization\n",
    "    lemmatized_ = [[token.split(':')[0] for token in art] for art in lemmatized__]\n",
    "\n",
    "    # Joining tokens back together into corpus\n",
    "    cleaned_ = [' '.join(tokens) for tokens in lemmatized_]\n",
    "\n",
    "    # Returning new dataframe with lemmatized values\n",
    "    return pd.DataFrame(cleaned_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = lemmatizer(df_train, 'DESCRIPTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = lemmatizer(df_train, 'TITLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation of lemmatized columns\n",
    "df_adult = pd.concat([title, desc, df_train['PRICE'], df_train['SPAM_FLAG']], axis=1)\n",
    "df_adult.columns = ['TITLE','DESCRIPTION','PRICE','SPAM_FLAG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Using TF-IDF vectorizer and Multinomial Naive Bayes as a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data into train and test sets\n",
    "train_df, test_df = train_test_split(df_adult, test_size=0.3, random_state=42)\n",
    "\n",
    "# Defining pipeline steps and fitting classifier\n",
    "steps = [('tfidf', TfidfVectorizer()), ('cls', MultinomialNB())]\n",
    "pipe = Pipeline(steps=steps)\n",
    "pipe.fit(train_df['DESCRIPTION'], train_df['SPAM_FLAG'])\n",
    "\n",
    "y_pred = pipe.predict(test_df['DESCRIPTION'])\n",
    "y_true = test_df['SPAM_FLAG']\n",
    "\n",
    "# Returning results\n",
    "plt.figure(figsize=(2,2))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred), square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value');\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    description_val = lemmatizer(df_val, 'DESCRIPTION')\n",
    "    title_val = lemmatizer(df_val, 'TITLE')\n",
    "    \n",
    "    df_adult_val = pd.concat([title_val, description_val, df_val['PRICE'], df_val['SPAM_FLAG']], axis=1)\n",
    "    df_adult_val.columns = ['TITLE','DESCRIPTION','PRICE','SPAM_FLAG']\n",
    "    \n",
    "    y_pred = pipe.predict(df_adult_val['DESCRIPTION'])\n",
    "    y_true = df_adult_val['SPAM_FLAG']\n",
    "\n",
    "    plt.figure(figsize=(2,2))\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred), square=True, annot=True, cbar=False)\n",
    "    plt.xlabel('predicted value')\n",
    "    plt.ylabel('true value');\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "except NameError:\n",
    "    print('No such dataframe. Check name and proper path to file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras with word2vec embeddings\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
